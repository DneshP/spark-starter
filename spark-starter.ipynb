{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apache spark - starter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "def getStateMnMColorCount(file, state=\"getAll\"):\n",
    "    \n",
    "    spark = (SparkSession\n",
    "             .builder\n",
    "             .appName(\"MnMColorCounter\")\n",
    "             .getOrCreate()\n",
    "            )\n",
    "    # Load the content\n",
    "    mnm_df = spark.read.format(\"csv\").option(\"header\", True).option(\"inferSchema\", True).load(file)\n",
    "    \n",
    "    # Structure of the data {state, color, count}\n",
    "    # groupby state and aggregate the mnm_color count\n",
    "    # query: SUM(count) FROM data GROUP BY state, mnm_color ORDER DESC\n",
    "    counts_df = mnm_df.select(\"State\", \"Color\", \"Count\").groupBy(\"State\", \"Color\").sum(\"Count\").orderBy(\"sum(Count)\", ascending=False)\n",
    "    if (state != \"getAll\"):\n",
    "        counts_df = counts_df.where(counts_df.State == state)\n",
    "    \n",
    "    print(counts_df.show(n=10, truncate=False))\n",
    "    \n",
    "    spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+----------+\n",
      "|State|Color |sum(Count)|\n",
      "+-----+------+----------+\n",
      "|CA   |Yellow|100956    |\n",
      "|WA   |Green |96486     |\n",
      "|CA   |Brown |95762     |\n",
      "|TX   |Green |95753     |\n",
      "|TX   |Red   |95404     |\n",
      "|CO   |Yellow|95038     |\n",
      "|NM   |Red   |94699     |\n",
      "|OR   |Orange|94514     |\n",
      "|WY   |Green |94339     |\n",
      "|NV   |Orange|93929     |\n",
      "+-----+------+----------+\n",
      "only showing top 10 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "getStateMnMColorCount(\"data/dummy-mnm.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset: https://catalog.data.gov/dataset/airline-on-time-performance-and-causes-of-flight-delays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+------------------+\n",
      "|min_delay|max_delay|         avg_delay|\n",
      "+---------+---------+------------------+\n",
      "|     -112|     1642|12.079802928761449|\n",
      "+---------+---------+------------------+\n",
      "\n",
      "+-----+--------+------+-----------+------------+\n",
      "|delay|    date|origin|destination|delay_status|\n",
      "+-----+--------+------+-----------+------------+\n",
      "|    6|01011245|   ABE|        ATL|  Acceptable|\n",
      "|   -8|01020600|   ABE|        DTW|  Early bird|\n",
      "|   -2|01021245|   ABE|        ATL|  Early bird|\n",
      "|   -4|01020605|   ABE|        ATL|  Early bird|\n",
      "|   -4|01031245|   ABE|        ATL|  Early bird|\n",
      "|    0|01030605|   ABE|        ATL|    Right on|\n",
      "|   10|01041243|   ABE|        ATL|  Acceptable|\n",
      "|   28|01040605|   ABE|        ATL|  Acceptable|\n",
      "|   88|01051245|   ABE|        ATL|  Long delay|\n",
      "|    9|01050605|   ABE|        ATL|  Acceptable|\n",
      "+-----+--------+------+-----------+------------+\n",
      "\n",
      "+-----+--------+------+-----------+\n",
      "|delay|    date|origin|destination|\n",
      "+-----+--------+------+-----------+\n",
      "| 1642|03090615|   TPA|        DFW|\n",
      "| 1638|02190925|   SFO|        ORD|\n",
      "| 1636|02021245|   FLL|        DFW|\n",
      "| 1592|03020700|   RSW|        ORD|\n",
      "| 1560|01180805|   BNA|        DFW|\n",
      "| 1553|03031210|   PDX|        DFW|\n",
      "| 1543|03070645|   CLE|        DFW|\n",
      "| 1511|02210630|   MCO|        ORD|\n",
      "| 1500|01300915|   EGE|        JFK|\n",
      "| 1496|01150715|   ONT|        DFW|\n",
      "+-----+--------+------+-----------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+-----+--------+------+-----------+------------+\n",
      "|delay|    date|origin|destination|delay_status|\n",
      "+-----+--------+------+-----------+------------+\n",
      "| 1642|03090615|   TPA|        DFW|  Long delay|\n",
      "| 1638|02190925|   SFO|        ORD|  Long delay|\n",
      "| 1636|02021245|   FLL|        DFW|  Long delay|\n",
      "| 1592|03020700|   RSW|        ORD|  Long delay|\n",
      "| 1560|01180805|   BNA|        DFW|  Long delay|\n",
      "| 1553|03031210|   PDX|        DFW|  Long delay|\n",
      "| 1543|03070645|   CLE|        DFW|  Long delay|\n",
      "| 1511|02210630|   MCO|        ORD|  Long delay|\n",
      "| 1500|01300915|   EGE|        JFK|  Long delay|\n",
      "| 1496|01150715|   ONT|        DFW|  Long delay|\n",
      "+-----+--------+------+-----------+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "from pyspark.sql.functions import col, desc, when\n",
    "\n",
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .appName(\"SQLSample\")\n",
    "         .getOrCreate()\n",
    "        )\n",
    "file = \"data/flight-delays.csv\"\n",
    "\n",
    "schema = StructType([\n",
    "   StructField(\"date\", StringType()),\n",
    "   StructField(\"delay\", IntegerType()),\n",
    "   StructField(\"distance\", IntegerType()),\n",
    "   StructField(\"origin\", StringType()),\n",
    "   StructField(\"destination\", StringType()),\n",
    "])\n",
    "fd_df = spark.read.format('csv').option('header', True).schema(schema).load(file)\n",
    "fd_df.createOrReplaceTempView(\"flight_delay_tbl\")\n",
    "\n",
    "# Describe the delays\n",
    "spark.sql(\"\"\"SELECT MIN(delay) as min_delay, MAX(delay) as max_delay, AVG(delay) as avg_delay FROM flight_delay_tbl\"\"\").show()\n",
    "\n",
    "spark.sql(\"\"\"SELECT delay, date, origin, destination,\n",
    "                CASE\n",
    "                    WHEN delay > 30 THEN 'Long delay'\n",
    "                    WHEN delay BETWEEN 1 AND 29 THEN 'Acceptable'\n",
    "                    WHEN delay = 0 THEN 'Right on'\n",
    "                    ELSE 'Early bird'\n",
    "                END AS delay_status\n",
    "                FROM flight_delay_tbl \n",
    "                DESC LIMIT 10\"\"\").show()\n",
    "\n",
    "fd_df.select(\"delay\", \"date\", \"origin\", \"destination\").where(\"delay > 30\").orderBy(\"delay\", ascending=False).show(10)\n",
    "fd_df.select(\"delay\", \"date\", \"origin\", \"destination\").withColumn(\"delay_status\", when(fd_df[\"delay\"] > 30, \"Long delay\")\n",
    ".when((fd_df[\"delay\"] > 0) & (fd_df[\"delay\"] < 30), \"Acceptable\").when(fd_df[\"delay\"] == 0, \"Right on\",).otherwise(\"Early bird\")).orderBy(\"delay\", ascending=False).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+-----------+\n",
      "|delay|    date|destination|\n",
      "+-----+--------+-----------+\n",
      "|   14|01010900|        LAX|\n",
      "|   -3|01011200|        LAX|\n",
      "|    2|01011900|        LAX|\n",
      "|   11|01011700|        LAS|\n",
      "|   -1|01010800|        SFO|\n",
      "|   -4|01011540|        DFW|\n",
      "|    5|01011705|        SAN|\n",
      "|   -3|01011530|        SFO|\n",
      "|   -3|01011630|        SJU|\n",
      "|    2|01011345|        LAX|\n",
      "+-----+--------+-----------+\n",
      "only showing top 10 rows\n",
      "\n",
      "DataFrame[date: string, delay: int, distance: int, origin: string, destination: string]\n"
     ]
    }
   ],
   "source": [
    "# Creating a separate database (spark managed)\n",
    "\n",
    "# managed db metadata spark.sql.warehouse.dir\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS flight_delay_db\")\n",
    "\n",
    "# fd_df.write.saveAsTable(\"flight_delay_tbl\")\n",
    "\n",
    "# unmanaged\n",
    "# fd_df.write.option(\"path\", \"/tmp/data/flight_delay\").mode(\"overwrite\").saveAsTable(\"flight_delay_tbl_\")\n",
    "\n",
    "# creating global and local views\n",
    "\n",
    "jfk = fd_df.select(\"delay\", \"date\", \"destination\").where(\"origin == 'JFK'\")\n",
    "jfk.createOrReplaceGlobalTempView(\"origin_jfk_global_tmp_view\")\n",
    "jfk.createOrReplaceTempView(\"origin_jfk_tmp_view\")\n",
    "\n",
    "# prefix global_temp to access global views\n",
    "tmp_view = spark.sql(\"SELECT * FROM global_temp.origin_jfk_global_tmp_view\")\n",
    "tmp_view.show(10)\n",
    "\n",
    "# catalog has the metadata information of the created view\n",
    "dbs = spark.catalog.listDatabases()\n",
    "tables = spark.catalog.listTables()\n",
    "columns = spark.catalog.listColumns(\"flight_delay_tbl\")\n",
    "\n",
    "# drop \n",
    "spark.catalog.dropGlobalTempView(\"origin_jfk_global_tmp_view\")\n",
    "spark.catalog.dropTempView(\"origin_jfk_tmp_view\")\n",
    "\n",
    "# read tables into dataframes\n",
    "data_from_table = spark.table(\"flight_delay_tbl\")\n",
    "print(data_from_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 19:====================================>                     (5 + 3) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+\n",
      "|age|    name|sex|\n",
      "+---+--------+---+\n",
      "| 26|test-one|  f|\n",
      "| 24|    test|  m|\n",
      "+---+--------+---+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# save dataframe as a parquet file\n",
    "\n",
    "sample = [{\n",
    "    \"name\": \"test\",\n",
    "    \"age\": 24,\n",
    "    \"sex\": \"m\"\n",
    "},{\n",
    "    \"name\": \"test-one\",\n",
    "    \"age\": 26,\n",
    "    \"sex\": \"f\"\n",
    "}]\n",
    "data_frame = spark.createDataFrame(sample)\n",
    "\n",
    "(data_frame\n",
    "     .write\n",
    "     .option(\"path\", \"tmp/data/parquet\")\n",
    "     .option(\"compression\", \"snappy\")\n",
    "     .mode(\"overwrite\")\n",
    "     .save()\n",
    ")\n",
    "\n",
    "# load parquet files\n",
    "revive = spark.read.load(\"tmp/data/parquet\")\n",
    "revive.show()\n",
    "# Avro - is used by Kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- image: struct (nullable = true)\n",
      " |    |-- origin: string (nullable = true)\n",
      " |    |-- height: integer (nullable = true)\n",
      " |    |-- width: integer (nullable = true)\n",
      " |    |-- nChannels: integer (nullable = true)\n",
      " |    |-- mode: integer (nullable = true)\n",
      " |    |-- data: binary (nullable = true)\n",
      " |-- label: integer (nullable = true)\n",
      "\n",
      "+------+-----+---------+\n",
      "|height|width|nChannels|\n",
      "+------+-----+---------+\n",
      "|   288|  384|        3|\n",
      "|   288|  384|        3|\n",
      "|   288|  384|        3|\n",
      "|   288|  384|        3|\n",
      "|   288|  384|        3|\n",
      "+------+-----+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read images\n",
    "\n",
    "image_dir = \"data/images/cctv_train_images/\"\n",
    "image_df = spark.read.format(\"image\").load(image_dir)\n",
    "image_df.printSchema()\n",
    "\n",
    "image_df.select(\"image.height\", \"image.width\", \"image.nChannels\").show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'numbers' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m schema \u001b[38;5;241m=\u001b[39m StructType([StructField(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumber\u001b[39m\u001b[38;5;124m\"\u001b[39m, IntegerType(), \u001b[38;5;28;01mTrue\u001b[39;00m)])\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Create a DataFrame with the specified schema\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m numbers_df \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mcreateDataFrame([(number,) \u001b[38;5;28;01mfor\u001b[39;00m number \u001b[38;5;129;01min\u001b[39;00m \u001b[43mnumbers\u001b[49m], schema)\n\u001b[1;32m     16\u001b[0m numbers_df\u001b[38;5;241m.\u001b[39mwrite\u001b[38;5;241m.\u001b[39mmode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverwrite\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39msaveAsTable(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumbers_tbl\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# extract data from table and pass it to the udf and get the output\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'numbers' is not defined"
     ]
    }
   ],
   "source": [
    "# Spark SQL UDFs\n",
    "# enable seamless integration of complex operations, \n",
    "# like machine learning models, in Spark SQL, simplifying data querying \n",
    "from pyspark.sql.types import LongType\n",
    "\n",
    "# define and register the udf\n",
    "def cube(n):\n",
    "    return n * n * n\n",
    "\n",
    "spark.udf.register(\"cube\", cube, LongType())\n",
    "\n",
    "schema = StructType([StructField(\"number\", IntegerType(), True)])\n",
    "\n",
    "# Create a DataFrame with the specified schema\n",
    "numbers_df = spark.createDataFrame([(number,) for number in numbers], schema)\n",
    "numbers_df.write.mode(\"overwrite\").saveAsTable(\"numbers_tbl\")\n",
    "\n",
    "# extract data from table and pass it to the udf and get the output\n",
    "spark.sql(\"SELECT number, cube(number) as predicted FROM numbers_tbl\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Panda UDF\n",
    "\n",
    "# Import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Import various pyspark SQL functions including pandas_udf\n",
    "from pyspark.sql.functions import col, pandas_udf\n",
    "from pyspark.sql.types import LongType\n",
    "\n",
    "def cubed(a: pd.Series) -> pd.Series:\n",
    "    return a * a * a\n",
    "\n",
    "# create panda udf\n",
    "cubed_udf = pandas_udf(cubed, returnType=LongType())\n",
    "\n",
    "# panda series \n",
    "x = pd.Series([1, 2, 3])\n",
    "cubed(x)\n",
    "\n",
    "data_frame = spark.range(1, 5)\n",
    "data_frame.select(\"id\", cubed_udf(col(\"id\"))).show()\n",
    "\n",
    "# using higher order functions such as explode, collect_list might cause out of memory errors when dealing with larger datasets\n",
    "# udf are safer in these situations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading from external source\n",
    "# sample\n",
    "query = \"SELECT c.colA, c.coln FROM c WHERE c.origin = 'SEA'\"\n",
    "readConfig = {\n",
    "  \"Endpoint\" : \"https://[ACCOUNT].documents.azure.com:443/\", \n",
    "  \"Masterkey\" : \"[MASTER KEY]\",\n",
    "  \"Database\" : \"[DATABASE]\",\n",
    "  \"preferredRegions\" : \"Central US;East US2\",\n",
    "  \"Collection\" : \"[COLLECTION]\",\n",
    "  \"SamplingRatio\" : \"1.0\",\n",
    "  \"schema_samplesize\" : \"1000\",\n",
    "  \"query_pagesize\" : \"2147483647\",\n",
    "  \"query_custom\" : query\n",
    "}\n",
    "\n",
    "cosmo_df = (\n",
    "spark\n",
    "    .read\n",
    "    .format(\"com.microsoft.azure.cosmosdb.spark\")\n",
    "    .option(**readConfig)\n",
    "    .load()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Join strategies\n",
    "Broadcast Hash Join (BHJ) \n",
    "    -> send the samller dataset as the broadcast variable to the larger dataset side, provided there is enough memory on the driver and executor\n",
    "Shuffle Hash Join (SHJ)\n",
    "Shuffle sort merge Join (SMJ)\n",
    "    -> when we have two large datasets and would like to merge based on a common key\n",
    "    equi joins, it is a efficient if the two datasets are bucketed by the merge key as this would avoid exchange \n",
    "Broadcast Nested Loop Hash Join (BLHJ)\n",
    "Shuffle and replicate nested loop Hash Join (SRNHJ)\n",
    "\n",
    "using the metrics to analyse the job, stages and task to make improvements\n",
    "\n",
    "# Streaming structure\n",
    "* state-less\n",
    "* state-full\n",
    "\n",
    "-> under statefull there are two types managed and unmanaged\n",
    "### Managed:\n",
    "* Streaming aggregations\n",
    "* Stream - stream joins\n",
    "* Streaming deduplicationn\n",
    "### Unmanaged - need you to define cleanup\n",
    "* MapGroupsWithState\n",
    "* FlatMapGroupsWithState\n",
    "\n",
    "say each sensor is expected to send at most one reading per minute and we want to detect if any sensor is reporting an unusually high number of times.\n",
    "To find such anomalies, we can count the number of readings received from each sensor in five-minute intervals.\n",
    "\n",
    "id, reading, created_at\n",
    "1, 1, now()\n",
    "1, 1, now()\n",
    "2, 1, now()\n",
    "3, 1, now()\n",
    "1, 1, now() + 5min\n",
    "2, 1, now() + 5min\n",
    "3, 1, now() + 5min\n",
    "\n",
    "SELECT * FROM df Group by id orderBy created_at ASC\n",
    "\n",
    "get the last created_at and traves down by aggregating count for interval of 5\n",
    "\n",
    "for streams create interval windows and update according to the created_at value and compute the value\n",
    "\n",
    "* window(col, value) -> dynamically grouped computed value\n",
    "* window(col, time_frame, sliding_value) -> resource extensive as the group does not know when to stop, adding withWatermark(col, time_frame) here the group will conclude after the watermark delay.\n",
    "\n",
    "# Stream - Static joins\n",
    "# Stream - Stream joins\n",
    "\n",
    "adImpressionSchema = id, content, impression_time\n",
    "clickSchema = id,  ad_id, click_time\n",
    "\n",
    "adImpressionStream = spark.readStream...\n",
    "clickStream = spark.readStream..\n",
    "\n",
    "adImpressionWithWatermark = adImpressionStream.withWaterMark('impression_time', 10 minutes).selectExpr('id as impressionId', 'content')\n",
    "clickStreamWithWatermark = adImpressionStream.withWaterMark('click_time', 10 minutes).selectExpr('id as ClickId', 'ad_is as clickedImpressionId')\n",
    "\n",
    "\n",
    "\n",
    "# the following is resource heavy since it unbounded\n",
    "adImpressionStream.join(clickStream).expr(\"\"\" ad_id = id \"\"\")\n",
    "# optimise by providing watermark (say the margin is set from the ad created time)\n",
    "adImpressionWithWatermark.join(clickStreamWithWatermark).expr(\n",
    "\"\"\"\n",
    "impressionId = clickedImpressionId\n",
    "click_time BETWEEN impression_time AND impression_time + inteval 1 hour\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "impression_time + inteval 1 hour will define the buffer time\n",
    "\n",
    "# Modeling arbitary stateful operations with mapGroupsWithState\n",
    "\n",
    "If we need to trigger some custom actions based on the stream input.\n",
    "we can have a udf and pass it to the mapGroupsWithState which expects a key, input stream, previous state\n",
    "the custom function can process based on the input data.\n",
    "If the input is triggered based on events and there might be cases where there could be inactivity on the key and this might cause the state to consume more resources\n",
    "* setting a timeout can mitigate this issue, after a timeout during the next micro batch call this key will be trigeered and the GroupState.hasTimedOut returns true and this can be used to remove the key from the state.\n",
    "* use the system processing time timeouts, where the system time will be used to trigger/call timeout keys\n",
    "\n",
    "# flatMapGroupsWithState\n",
    "Overcomes the limitation where the result can be a empty as apposed to mapGroupsWithState, it returns a iterator\n",
    "\n",
    "# Performance tuning for stream queries\n",
    "* since the resources are kept running 24/7 managing resources is a crucial step\n",
    "* cluster resource management(the number of cores)\n",
    "* Unlike batch processing the executors deal with little amounts of data so, the shuffle partition, rate limiting if there is a sudden spike in the incoming data, this will move the the data to buffer and process accordingly\n",
    "\n",
    "# Databases\n",
    "* Online transaction processing workloads\n",
    "* Online analytical processing workloads\n",
    "\n",
    "Data lake is a distributed storage but does not guarantee ACID\n",
    "Data lakehouse overcomes the problems posed by Datalake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Lakehouses with Apache Spark and Delta Lake\n",
    "* Dataset - https://www.kaggle.com/datasets/wordsforthewise/lending-club"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise the spark application\n",
    "\n",
    "import sys\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "deltaSpark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:3.0.0\")\n",
    "    .appName(\"deltaSpark\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "# Load the dataset and write to table\n",
    "\n",
    "filePath = \"data/loan-risks.snappy.parquet\"\n",
    "deltaTmpDir = \"tmp/risky-loans/\"\n",
    "\n",
    "# read from file and write to delta format\n",
    "(deltaSpark\n",
    "     .read\n",
    "     .format(\"parquet\") # default format\n",
    "     .load(filePath)\n",
    "     .write\n",
    "     .format(\"delta\")\n",
    "     .mode(\"overwrite\")\n",
    "     .save(deltaTmpDir)\n",
    ")\n",
    "# creating a view on the data \n",
    "\n",
    "# (deltaSpark\n",
    "     # .read\n",
    "     # .format(\"delta\")\n",
    "     # .load(deltaTmpDir)\n",
    "     # .createOrReplaceTempView(\"loans_delta\")\n",
    "# )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
